Lecture 2: Data Center Networking– Basics, Topology

Topology
or an evolution of Google's network topologies
Credit: A. Singh et al., “Jupiter rising: A decade of Clos topologies and centralized control in Google's datacenter network,” ACM SIGCOMM'15.
2004: four-post cluster
Supported 20k servers per cluster
The diagram shows a four-post cluster. There are four cluster routers, each of which is connected to two ToR switches. The ToR switches are then connected to the servers. Each server has two 10GbE connections to the ToR switches. But traffic keeps growing
The diagram shows the growth of traffic generated by servers in Google's datacenters. The traffic has been growing exponentially since 2008. This growth is likely due to the increasing number of people using Google's services, such as Search, Gmail, and YouTube. The growth in traffic is also likely due to the increasing use of mobile devices, which are constantly generating data.

How to scale the network
- Buying the largest switches with the most ports doesn't scale well
- And it's expensive
- Solution: Clos topologies
The diagram shows a Clos topology. A Clos topology is a hierarchical network design that is used to scale networks. The Clos topology consists of three stages: the spine stage, the aggregation stage, and the edge stage. The spine stage is the core of the network and it is responsible for connecting the aggregation switches. The aggregation stage is responsible for connecting the edge switches to the spine switches. The edge stage is responsible for connecting the servers to the aggregation switches.
A practical approach to scale
- Use Clos topologies:
  - Use many low-radix switches in multiple stages to scale to arbitrary size
  - Substantial path diversity and redundancy
- Merchant silicons
  - off-the-shelf, allows regular and rapid upgrades

---

[2005] Firehose 1.0
Spine Block
32x10G to 32 aggregation blocks
Aggregation Block
(32x10G to 32 spine blocks)
Stage 2, 3 or 4 board
ToR (Stage 1) board: 2x10G up, 24x1G down
Stage 2, 3, 4 board: 4x10G up, 4x10G down
Stage 5 board: 8x10G down
An aggr block has 16 ToRs (320 machines). 32 spine blocks each connect to 32 aggr blocks, resulting in a fabric that scales to 10K machines with 1G average bandwidth

2012 Jupiter
Jupiter is a large-scale, shared-memory NUMA system designed at IBM Research. It is based on a 3-level hierarchical topology. The top level consists of 8 NUMA nodes, each with 32 cores and 64GB of memory. The middle level consists of 16 compute nodes, each with 8 cores and 32GB of memory. The bottom level consists of 64 memory nodes, each with 4GB of memory. The system is interconnected with a high-speed network that provides 10GB/s of bandwidth between each pair of nodes.
The diagram shows the system architecture of Jupiter. The top level consists of 8 NUMA nodes, each with 32 cores and 64GB of memory. The middle level consists of 16 compute nodes, each with 8 cores and 32GB of memory. The bottom level consists of 64 memory nodes, each with 4GB of memory. The system is interconnected with a high-speed network that provides 10GB/s of bandwidth between each pair of nodes.

Facebook's fabric
With 96 pods, the topology can accommodate 73,728 10Gbps hosts. In Facebook's Altoona data center, each aggregation switch connects to 48 ToR switches in its pod, and 12 out of the 48 possible core switches on its plane, resulting in a 4:1 oversubscription.
The diagram shows the Facebook fabric, which is a network topology that is used to connect the servers in Facebook's data centers. The fabric is designed to be scalable, reliable, and efficient.
The fabric is made up of three layers:
- The core layer is made up of a number of core switches that are connected to each other in a mesh topology. The core switches are responsible for routing traffic between the different pods in the fabric.
- The aggregation layer is made up of a number of aggregation switches that are connected to the core switches. The aggregation switches are responsible for aggregating traffic from the ToR switches in the pods and sending it to the core switches.
- The ToR layer is made up of a number of ToR switches that are connected to the aggregation switches. The ToR switches are responsible for connecting the servers in the pods to the fabric.
The fabric is designed to be scalable so that it can be easily expanded to accommodate more servers. The fabric is also designed to be reliable so that it can withstand the failure of individual switches or links. The fabric is also designed to be efficient so that it can handle a large amount of traffic without experiencing congestion.

---

Academia: Fat-tree
The Clos topology is built upon commodity switches. The diagram shows a Clos topology with three levels: core, aggregation, and edge. The core level consists of a single switch that connects all the aggregation switches. The aggregation level consists of two switches that connect the edge switches to the core switch. The edge level consists of four switches that connect the hosts to the aggregation switches.
The Clos topology is a scalable, commodity data center network architecture. It is scalable because it can be easily expanded to support more hosts. It is also cost-effective because it uses commodity switches. The Clos topology is a popular choice for data centers because it provides high performance and scalability.

Fat-tree
A k-pod fat-tree has k pods, each with two layers of k/2 switches. Each aggregation switch has k/2 ports to unique core switches and k/2 aggregation switches in a pod. So the total number of core switches is (k/2)².
The diagram shows a 4-pod fat-tree. Each pod has two layers of 2 switches each. The first layer of switches is connected to the hosts. The second layer of switches is connected to the core switches. The core switches are connected to each other in a full mesh.
- Each core switch has one port to each pod.
- Each edge switch has k/2 hosts. k/2 edge switches in each of k pods. So a k-pod fat-tree can support k³/4 hosts.
The diagram shows a fat-tree network topology. A fat-tree network is a type of data center network that is designed to provide high scalability and performance. The network is made up of multiple layers of switches, with each layer connecting to the next layer in a tree-like structure. The top layer of the network is made up of core switches, which are connected to each other in a full mesh. The next layer down is made up of aggregation switches, which are connected to the core switches in a star topology. The bottom layer of the network is made up of edge switches, which are connected to the aggregation switches in a tree topology.
The fat-tree network topology is designed to provide high scalability and performance by distributing traffic across multiple paths. This helps to reduce congestion and improve the overall performance of the network. The network is also designed to be fault-tolerant, so that if one or more switches fail, the network can still continue to operate.

Advantages of fat-tree
In traditional hierarchical networks, switches in aggregation and core layers need to be more powerful and have more ports per device. High-end, high port density switches are extremely expensive.
Scale out vs. scale up
Fat-tree: (5k2/4) k-port switches support k^3/4 hosts
48-port 1GigE switches: 27,648 hosts using 2,880 switches.
Advantages of fat-tree
- Rearrangeably non-blocking: for arbitrary communication patterns, there is some set of paths that will saturate all the bandwidth available to the end hosts in the topology.
- Hierarchical topologies are not rearrangeably non-blocking.
